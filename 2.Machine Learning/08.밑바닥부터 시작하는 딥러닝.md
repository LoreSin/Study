# 밑바닥부터 시작하는 딥러닝
    아나콘다(pagake environment) + 주피터(IDE)

[소스코드 다운로드](https: // github.com / oreilly - japan / deep - learning - from-scratch)


## 1.머신러닝 개요

- AI(인공지능) 중에 ML(머신러닝) 중에 인공신경망 의 DL(딥러닝)이다.
- ML 은 지도학습, 비지도학습, 강화학습등이 있으며, 심화학습은 이러한 유형중 하나


## 파이썬 주요 라이브러리
    numpy: 행렬 및 다차원 연산
    matplotlib.pyplot: 파이썬 수학그래프 출력
    opencv - python: opencv의 파이썬용 라이브러리(사용시 import cv2)
    PIL: 파이썬 이미지 처리 라이브러리(Python Image Library)

## 2.퍼셉트론(Perceptron)
    논리회로와 비슷한 연산(AND OR NAND 등) 을 연산하는 방법.
    W계수(가중치) 를 각 입력마다 곱하여, 출력에 가중되는 정도의 차이를 둔다.


> x1, x2 입력 | w1, w2 입력별 가중치 | θ theta 기준치 | y 출력

> y = 0: : x1w1 + x2w1 <= θ

> y = 1: : x1w1 + x2w1  >  θ


* θ를 - b 로 바꾸어(bias
               편향치) 출력을 0을 기준으로 설정하는 것을 퍼셉트론이라 한다.

> x1, x2 입력 | w1, w2 입력별 가중치 | θ = > b(bias) 편향 | y 출력

> y = 0: : x1w1 + x2w1 + b <= 0

> y = 1: : x1w1 + x2w1 + b >   0

#### 예시: **AND** 연산
    w1, w2 = 0.5, 0.5
    b = -0.7 일때

|입력 x1 | 입력 x2 | 출력 y | 식 계산값 |
|--: | --: | --: | --: |
|0 | 0 | 0 | -0.7 (0 이하는 0 출력)|
|0 | 1 | 0 | -0.2 (0 이하는 0 출력)|
|1 | 0 | 0 | -0.2 (0 이하는 0 출력)|
|1 | 1 | 1 | 0.3 (0 이상은 1 출력)|

---


# 3.신경망
    퍼셉트론 + 활성화 함수(Activation Functions)을 다중으로 사용 = > 신경망
    h() = 활성화 함수

# 활성화 함수
- 퍼셉트론의 출력값을, 0~1범위 이내의 값으로 출력을 정리함.
- 1이상 값은 1, 0 이하의 값은 0으로 만든다.
- 신경망에서는 활성화 함수를 포함한 퍼셉트론을 사용.
- 특히 다중으로 중첩된 퍼셉트론에서는 ** 비선형 함수**가 중요

1. 선형함수들
    * 계단함수(Step Function): 출력을 0을 기준으로 0 or 1 만을 출력
        - y = 1 if x > 0 else 0
    * 시그모이드 함수(Sigmoid Function): 출력이 곡선형으로 분포되며 출력
        - y = 1 / (1 + math.exp(-x))

    ![활성화 함수](. / activation_Function.png)

2. 비선형 함수
    * **ReLU 함수 ** (Rectifide Linear Unit): 0을 기준으로 위로는 x 값 그대로 출력
        - y = x if x > 0 else 0

    ![활성화 함수](. / RELU_Function.png)


# 다중 신경망과 행렬연산
    다층 신경망은 행렬곱으로 표현하고, numpy 연산으로도 가능하다.

1. numpy 행렬곱
    - np.dot(A, B)
    행렬의 차원이 맞아야 한다. (4 * 3 과 3 * 2 유형)
    - A.dot(B)
    위의 것과 같은 연산. (A 행렬이 바뀌지 않고, 계산된 행렬반환)

2. 다중 신경망
    - 신경망의 각 계층에 맞는 W(가중치), b(편향치) 의 행렬을 작성
    - 각 계층에 따라 해당 행렬을 연산하고, 활성화 함수를 불러 계층씩 뉴런을 계산

> X, Y, Z는 입력, 출력, 은닉층(중간층) 으로 가칭
>
> Z or Y = XW + b
와 같이 하나의 계층연산을 표현.
>
> a = X.dot(W) + b 로 계산
>
> Z or Y = sigmoid(a)
활성화 함수로 신경망 1개 계층 연산완료.
>
> 신경망 1계층의 행렬식은 ** Z or Y = activationFunc(X.dot(W) + b)**


# 출력층 설계
    분류(Classification) 및 회귀(Regression) 으로 만드는 과정
    분류: 출력값이 특정한 무리에 속하는지를 확정
    회귀: 기존 입력값으로 다음 값의 경향을 예측하는것

- 회귀 = 항등함수
- 2클래스 분류 = 시그모이드(Sigmoid func)
- 다중 클래스분류 = 소프트맥스 함수(Softmax func)

[붓꽃 데이터셋](https://en.wikipedia.org/wiki/Iris_flower_data_set)


1. 소트프맥스 함수
    - Yk = exp(Ak) / Σexp(ai)
    - exp (자연로그의 밑수)를 말함.[exp 내용](https: // ko.wikipedia.org / wiki / E_( % EC%83%81%EC%88%98))
    - 이러한 식을 사용하면, 모든 구성원의 합은 1이 되어, 각 요소를 ** 확률 ** 로 취급할수 있다.
    - **단조증가**함수 이기 때문에, 각 원소간 ** 대소관계는 유지**.
    - 입력치가 크면 컴퓨터의 표현량을 넘어, 제대로 연산되지 않음.

![소트프맥스 연산 실패](SoftmaxError.png)


- 그러한 식을 보정하여, 입력값을 보정한 값을 조정하여 연산 시킨다.
- exp(n) = 1 / log(n) 의 성질(역함수) 를 이용하여, 입력값들에 일정한 수를 더하여, 소프트맥스를 적용. (C' 으로 표기)
> - Yk = exp(Ak - **C`**) / Σexp(ai)

-  보통, 원소중 가장 높은 숫자를 C' 에 적용
-  입력값이 지나치게 높을때, 숫자의 범위를 전체적으로 낮추어 연산.
-  현업에서는 연산자원의 이유로 일반적으로 생략

```py
def softmax(x):
    exp_x = np.exp(x)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y
```


2. 학습과 추론
    - 학습: **데이터**로 학습하여 ** 학습모델**을 만드는것
    - 추론: 학습모델을 이용하여 새로운 ** 데이터를 추측 ** 하는것.

3. Overfit(과적합)
    - 해당 데이터의 학습이 과도한 수준으로 이루어져, 추론이 제대로 되지 않는 경우.
    - 이런 경우, 학습모델 형성이 과도하게 되었다는 의미로 쓰임.

# 손글씨 숫자 인식 (추론과정 = 순전파 실습)
    이미 학습된 매개변수를 이용한 추론과정을 실습. (Inference)
    추론과정 = > 순전파 라고 부름 (Forward Propagation)
    MNIST 데이터셋 사용. (소스코드 내 dataset 폴더)

- 소스코드내의 dataset / mnist.py 을 가져와서, load_mnist 로 학습데이터등을 불러온다.


```py
from dataset.mnist import load_mnist
(x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=False)
# x_train = 6만개 이미지 학습용데이터
# t_train = 6만개의 숫자값.
# x_test = 1만개 이미지 추론용데이터
# t_test = 1만개의 숫자값.

```

- 이미 학습된 모델(sample_weight.pkl) 에서 Weight 및 Bias 값을 가지고 추론 모델을 생성

```py
def get_data():
    (x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=True, one_hot_label=False)
    return x_test, t_test

def get_data():
    (x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=True, one_hot_label=False)
    return x_test, t_test


x, t = get_data()
network = init_network() # 학습모델을 가지고 W 와 B 를 설정한 신경망 생성.

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i]) # 추론 함
    p = np.argmax(y) # 가장 높은 값의 인덱스 반환
    if p == t[i]: # 추정값과 설정값 비교
        accuracy_cnt += 1 # 맞으면 추가

print("Accuracy:" + str(float(accuracy_cnt) / len(x))) # 정확도 계산
```


- 전처리(pre-processing) : 입력데이터가 추론에 적합하게 만드는 변조
    + 정규화 (Normalization) : 입력값 (픽셀의 값 0~255) 을 특정 범위 이내 (0.0~1.0) 로 분포하게 변환처리 
    + 백색화 (Whitening) : 전체 데이터를 균일분포 처리




# 4. 신경망 학습
    신경망의 W (Weight) 와 B (Bias) 를 찾아내는 과정.
    입력데이터가 주어지고, 해당 입력데이터를 기준으로 신경망을 생성

#### 데이터 학습
    데이터를 가지고, 학습하여 사람의 개입을 최소화
    특징을 뽑아내어 학습을 시키는 방법 : 지도학습 (Supervised)
    입력데이터 그대로 신경망에 넣어 학습하는 방법 : 딥러닝 (Deep)


- 학습데이터에서 특징을 추출 => 지도학습기법 사용 
- 사람이 **특징(Feature)** 추출후 데이터는 벡터 (Vector) 로 기술 (SIFT, SURF, HOG)
- 학습시 기계가 벡터자료를 가지고 학습. (SVM, KNN 등)

#### 손실함수 (Loss Function)
    신경망 학습이 얼마나 잘 되었는가? 의 평가지표
    실제값 - 추정값의 오차를 계산한 값.
    MSE : Mean Square Error 평균제곱오차
    CEE : Cross Entropy Error 교차 엔트로피 오차

```py
def mean_square_error(y, t): #평균제곱 오차 MSE
    return 0.5*np.sum((y - t) ** 2)

def cross_entropy_error(y, t): # 교차 엔트로피 교차 CEE
    return -np.sum(t * np.log(y + delta))

```

- 미니배치 : 많은 훈련데이터중 무작위의 일부데이터로 훈련하는것. = 미니배치학습
- 미니배치학습을 하는 이유는 **손실함수** 로 일부 데이터로 먼저 검증.
- 정확도가 아닌 오류율을 사용하는 이유
    + 정확도로 할 경우, 기준값 대비 차이가 적어서 오류로 판정하기 어려움

#### 수치미분 (기울기:Gradient 선행지식)
    미분을 추정하는 것

- 아주 작은 구간 ( ex. x ± 1/1000 ) 의 변화량을 미분값으로 간주함.

```py
def num_diff(f, x): # 미분
    dist = 0e-4 
    ans = (f(x + dist) - f(x - dist)) / 2
    return ans
    # dist 는 컴퓨터에서 산술 가능한 가급적 적은 수. = 오차가 적어지고, 너무 크면 그래프의 특성이 더 많이 반영됨. 
    # (x 값의 dist 앞뒤의 값의 차) 의 평균
    # f식 (f 의 그래프선상) 에서 x ± dist 의 값차이의 평균
    # 결론 : 매우 짧은 구간의 변화량
```

#### 기울기 (Gradient)
    x = 입력, Ye = 추정치, Yt = 실제값, W = 현재(또는 초기) 가중치, f = Wx + b, lr = learning rate 학습률
    최소의 loss 값 (손실함수 값) 을 찾아내는 방법

- 우리는 다양한 출력값 (mnist 에서 0~9 의 출력값)
    + 0 ~ 9 모두의 오차값인 loss 값 (추정값)이 실제 0~9 까지의 값 (실제값)
    + 을 W 로 재 반영을 위해, 각 미분값의 Grad 를 구함.

```py
grad = sum(num_diff(f, Ye)) # 이 값은 해당 y 의 W 값을 구한다.
W -= grad * lr # 해당 값을 학습률 만큼 재 반영함.

```





#### 학습알고리즘 구현
    클래스로 학습 반복, 에폭 (epoch)

- 에폭 : 총 학습개수를 최소학습 단위 (배치학습) 으로 묶는 임의 단위
    + 예시) 6만개의 학습개수 / 100개의 배치학습단위 = 600 에폭 단위 가능.
    + 하지만, 랜덤 샘플 학습의 경우, 10000 / 100 이라 하더라도, 더 많은 단위로 학습 가능. (100 개의 개별 에폭이 존재하지만, 중복이 허용되면 ++ 가능.)


#####학습의 과정

1. y = Wx + b 이라는 선형회귀를 가정하고, 각 문제를 설정. (출력이 2중이든 다중이든)
2. 학습데이터 및 시험데이터 (통상 k~ 단위) 를 준비
    - 학습데이터의 입력자료 + 정답자료 (x_train, t_train) 을 준비
3. W 및 b 값을 무작위로 설정 (시작값)
4. x_train 과 t_train 의 case 를 W 와 b 의 loss (cost) 를 계산 (MSE, CEE)
    - 미니배치 (case 100갯수 = 1에폭)
5. 해당 loss 의 Gradient 를 구함. (Gradient 는 W 의 보정치로 생각함)
    - 기울기 산출
6. 학습률에 따라  Gradient 를 W 에 반영 한다. (W -= lr * grad)
    - 매개변수 갱신
7. 4 ~ 6 반복 함.


# 5. 오차역전법

#### 계산그래프
    그래프 이론 / 순전파와 역전파를 자료구조로 표현하는 방법
    계산그래프의 장점? 역전파의 미분이 효율적




